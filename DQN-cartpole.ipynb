{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8d0efHe42/1GJsQomPJqE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["INSTALLATION\n","--"],"metadata":{"id":"_-CFJJuq2ugn"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrPusugTjYO3","executionInfo":{"status":"ok","timestamp":1662184860346,"user_tz":-540,"elapsed":16811,"user":{"displayName":"youngwoo Oh","userId":"06892641019570208070"}},"outputId":"780c0644-3d64-455a-aa0b-2a8a8acec408"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.18.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.12.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.3.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.2.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.37.1)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.3.3)\n","Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.18.5)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.4.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.47.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.3.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.10.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.4.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.8.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.23.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.1.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2022.6.15)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow==2.3.0 in /usr/local/lib/python3.7/dist-packages (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.37.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.14.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.3.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.3.3)\n","Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.18.5)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.8.0)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.17.3)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.47.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.10.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.2.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.0)\n"]}],"source":["!pip install gym\n","!pip install keras\n","!pip install keras-rl2\n","!pip install tensorflow==2.3.0"]},{"cell_type":"markdown","source":["IMPORT LIB.\n","--"],"metadata":{"id":"voZ9VRul2x0H"}},{"cell_type":"code","source":["import sys\n","import gym\n","import math\n","import time\n","import pylab\n","import cmath\n","import random\n","import itertools\n","import threading\n","import tensorflow\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","from gym import Env\n","import scipy.io as sc\n","import tensorflow as tf\n","from sys import version\n","from absl import logging\n","from numpy import ndarray\n","from scipy import special\n","from gym.utils import seeding\n","from scipy.constants import *\n","from rl.agents import DQNAgent\n","from scipy.special import erfinv\n","from scipy.integrate import quad\n","from scipy.linalg import toeplitz\n","from numba import jit, njit, prange\n","from gym.spaces import Discrete, Box\n","from collections import deque, Counter\n","from rl.policy import BoltzmannQPolicy\n","from rl.memory import SequentialMemory\n","from mpl_toolkits.mplot3d import Axes3D\n","from gym import Env, error, spaces, utils\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten, Input\n","\n","# Set of times noew roman font.\n","from matplotlib import cm\n","from matplotlib import colors\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","plt.rcParams['font.family'] = 'serif'\n","plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n","\n","dtype = np.float32\n","\n","# import tensorflow.compat.v1 as tf\n","# tf.disable_v2_behavior()\n","\n","# import tensorflow as tf\n","# tf.compat.v1.disable_v2_behavior()\n","##############################################################################"],"metadata":{"id":"0r6YB2bHjb5c","executionInfo":{"status":"ok","timestamp":1662184902689,"user_tz":-540,"elapsed":4649,"user":{"displayName":"youngwoo Oh","userId":"06892641019570208070"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b2cbabf-05bb-43e4-d86e-1e2045ea4b85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  class IteratorBase(collections.Iterator, trackable.Trackable,\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  class DatasetV2(collections.Iterable, tracking_base.Trackable,\n"]}]},{"cell_type":"markdown","source":["AI-GYM CARTPOLE\n","--"],"metadata":{"id":"FCNyIxPnjkhl"}},{"cell_type":"code","source":["\"\"\"\n","Classic cart-pole system implemented by Rich Sutton et al.\n","Copied from http://incompleteideas.net/sutton/book/code/pole.c\n","permalink: https://perma.cc/C9ZM-652R\n","--------\n","Change action type from Discrete to Box.\n","\"\"\"\n","\n","import math\n","import gym\n","from gym import spaces, logger\n","from gym.utils import seeding\n","import numpy as np\n","\n","\n","class CartPoleEnv(gym.Env):\n","    \"\"\"\n","    Description:\n","        A pole is attached by an un-actuated joint to a cart, which moves along\n","        a frictionless track. The pendulum starts upright, and the goal is to\n","        prevent it from falling over by increasing and reducing the cart's\n","        velocity.\n","    Source:\n","        This environment corresponds to the version of the cart-pole problem\n","        described by Barto, Sutton, and Anderson\n","    Observation:\n","        Type: Box(4)\n","        Num   Observation               Min             Max\n","        0     Cart Position             -4.8            4.8\n","        1     Cart Velocity             -Inf            Inf\n","        2     Pole Angle                -24 deg         24 deg\n","        3     Pole Velocity At Tip      -Inf            Inf\n","    Actions:\n","        Type: Box(1)\n","        Num   Action                    Min             Max\n","        0     Input force               -1              1\n","    Reward:\n","        Reward is 1 for every step taken, including the termination step\n","    Starting State:\n","        All observations are assigned a uniform random value in [-0.05..0.05]\n","    Episode Termination:\n","        Pole Angle is more than 12 degrees.\n","        Cart Position is more than 2.4 (center of the cart reaches the edge of\n","        the display).\n","        Episode length is greater than 200.\n","        Solved Requirements:\n","        Considered solved when the average reward is greater than or equal to\n","        195.0 over 100 consecutive trials.\n","    \"\"\"\n","\n","    metadata = {\n","        'render.modes': ['human', 'rgb_array'],\n","        'video.frames_per_second': 50\n","    }\n","\n","    def __init__(self):\n","        self.gravity = 9.8\n","        self.masscart = 1.0\n","        self.masspole = 0.1\n","        self.total_mass = (self.masspole + self.masscart)\n","        self.length = 0.5  # actually half the pole's length\n","        self.polemass_length = (self.masspole * self.length)\n","        self.force_mag = 10.0\n","        self.tau = 0.02  # seconds between state updates\n","        self.kinematics_integrator = 'euler'\n","\n","        # Angle at which to fail the episode\n","        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n","        self.x_threshold = 2.4\n","\n","        # Angle limit set to 2 * theta_threshold_radians so failing observation\n","        # is still within bounds.\n","        high = np.array([self.x_threshold * 2,\n","                         np.finfo(np.float32).max,\n","                         self.theta_threshold_radians * 2,\n","                         np.finfo(np.float32).max], dtype=np.float32)\n","\n","        self.action_space = Discrete(2)\n","        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n","\n","        self.seed()\n","        self.viewer = None\n","        self.state = None\n","\n","        self.steps_beyond_done = None\n","\n","    def seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","\n","    def step(self, action):\n","        err_msg = \"%r (%s) invalid\" % (action, type(action))\n","        assert self.action_space.contains(action), err_msg\n","\n","        x, x_dot, theta, theta_dot = self.state\n","        force = self.force_mag * action\n","        costheta = math.cos(theta)\n","        sintheta = math.sin(theta)\n","\n","        # For the interested reader:\n","        # https://coneural.org/florian/papers/05_cart_pole.pdf\n","        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n","        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n","        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n","\n","        if self.kinematics_integrator == 'euler':\n","            x = x + self.tau * x_dot\n","            x_dot = x_dot + self.tau * xacc\n","            theta = theta + self.tau * theta_dot\n","            theta_dot = theta_dot + self.tau * thetaacc\n","        else:  # semi-implicit euler\n","            x_dot = x_dot + self.tau * xacc\n","            x = x + self.tau * x_dot\n","            theta_dot = theta_dot + self.tau * thetaacc\n","            theta = theta + self.tau * theta_dot\n","\n","        self.state = (x, x_dot, theta, theta_dot)\n","\n","        done = bool(\n","            x < -self.x_threshold\n","            or x > self.x_threshold\n","            or theta < -self.theta_threshold_radians\n","            or theta > self.theta_threshold_radians\n","        )\n","\n","        if not done:\n","            reward = 1.0\n","        elif self.steps_beyond_done is None:\n","            # Pole just fell!\n","            self.steps_beyond_done = 0\n","            reward = 1.0\n","        else:\n","            if self.steps_beyond_done == 0:\n","                logger.warn(\n","                    \"You are calling 'step()' even though this \"\n","                    \"environment has already returned done = True. You \"\n","                    \"should always call 'reset()' once you receive 'done = \"\n","                    \"True' -- any further steps are undefined behavior.\"\n","                )\n","            self.steps_beyond_done += 1\n","            reward = 0.0\n","\n","        return np.array(self.state), reward, done, {}\n","\n","    def reset(self):\n","        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n","        self.steps_beyond_done = None\n","        return np.array(self.state)"],"metadata":{"id":"PFGGaSowjnKg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CARTPOLE ENV TEST BEFORE TRAINING\n","--"],"metadata":{"id":"sGsYlXN838B1"}},{"cell_type":"code","source":["env = CartPoleEnv()\n","\n","episodes = 10\n","for episode in range(1, episodes+1):\n","    state = env.reset()\n","    done = False\n","    score = 0 \n","    \n","    while not done:\n","        action = env.action_space.sample()\n","        n_state, reward, done, info = env.step(action)\n","        score+=reward\n","    print('Episode:{} Score:{}'.format(episode, score))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9QyH7E239Xx","executionInfo":{"status":"ok","timestamp":1662184909675,"user_tz":-540,"elapsed":289,"user":{"displayName":"youngwoo Oh","userId":"06892641019570208070"}},"outputId":"a3b82bb9-b7a8-40d0-b0d9-e94741af2189"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode:1 Score:12.0\n","Episode:2 Score:13.0\n","Episode:3 Score:13.0\n","Episode:4 Score:11.0\n","Episode:5 Score:17.0\n","Episode:6 Score:10.0\n","Episode:7 Score:11.0\n","Episode:8 Score:12.0\n","Episode:9 Score:11.0\n","Episode:10 Score:10.0\n"]}]},{"cell_type":"markdown","source":["DQN MODEL\n","--"],"metadata":{"id":"_d6iannkjoE7"}},{"cell_type":"code","source":["# =============================================================================\n","# DEEP Q NETWORK (DQN)\n","# =============================================================================\n","\n","# THIS IS DQN AGENT FOR THE CUSTOM ENV.\n","# IT USES NEURAL NETWORK TO APPROXIMATE Q FUNCTION, AND REPLAY MEMORY & TAGET Q NETWORK\n","class DQNAgent:\n","    def __init__(self, state_size, action_size, test_mode):\n","\n","        # ARE YOU LOADING A PREVIOUS TRAINED DQN MODEL?\n","        # self.load_model = False\n","        self.load_model = test_mode\n","\n","        # GET SIZE OF STATE AND ACTION\n","        self.state_size = state_size\n","        self.action_size = action_size\n","\n","        # THESE ARE HYPER-PARAMERTERS FOR THE DQN\n","        self.discount_factor = 0.99\n","        self.learning_rate = 1e-3\n","        self.epsilon_decay = 0.999\n","        self.train_start = 1000\n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.01\n","        self.batch_size = 64\n","\n","        # CREATE REPLAY MEMORY USING DEQUE\n","        self.memory = deque(maxlen=50000)\n","\n","        # CREATE MAIN MODEL AND TARGET MODEL\n","        self.model = self.build_model()\n","        self.target_model = self.build_model()\n","\n","        # INITIALIZE TARGET MODEL\n","        self.update_target_model()\n","\n","        # GET TRAINED MY MODEL\n","        if self.load_model:\n","            self.model.load_weights(\".cartpole.h5\")\n","\n","    # APPROXIMATE Q FUNCTION USING NEURAL ENTWORK, STATE IS INOUT AND Q VALUE OF EACH ACTION IS OUTPUT OF NETWORK\n","    def build_model(self):\n","        model = Sequential()\n","        model.add(Dense(24, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n","        model.add(Dense(24, activation='relu', kernel_initializer='he_uniform'))\n","        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate), metrics=['accuracy'])\n","        return model\n","\n","    # AFTER SOME THIME INTERVAL UPDATE THE TARGET MODEL TO BE SAME WITH MODEL\n","    def update_target_model(self):\n","        self.target_model.set_weights(self.model.get_weights())\n","\n","\n","    # GET ACTION FROM MODEL USING EPSILON-GREEDY POLICY\n","    def get_action(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        else:\n","            q_value = self.model.predict(state)\n","            return np.argmax(q_value[0])\n","\n","    def predict_power(self, state):\n","      power = env.action_values[self.get_action(state)]\n","      return power\n","\n","\n","    # SAVE SAMPLE <STATE, ACTION, REWARD, NEXT STATE> TO THE REPLAY MEMORY\n","    def append_sample(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","\n","    # PICK SAMPLES RANDOMLY FORM REPLAY MEOMORY (WITH BATCH)\n","    def train_model(self):\n","        if len(self.memory) < self.train_start:\n","            return\n","        batch_size = min(self.batch_size, len(self.memory))\n","        mini_batch = random.sample(self.memory, batch_size)\n","\n","        update_input = np.zeros((batch_size, self.state_size))\n","        update_target = np.zeros((batch_size, self.state_size))\n","        action, reward, done = [], [], []\n","\n","        for i in range(self.batch_size):\n","            update_input[i] = mini_batch[i][0]\n","            action.append(mini_batch[i][1])\n","            reward.append(mini_batch[i][2])\n","            update_target[i] = mini_batch[i][3]\n","            done.append(mini_batch[i][4])\n","\n","        target = self.model.predict(update_input)\n","        target_val = self.target_model.predict(update_target)\n","\n","        for i in range(self.batch_size):\n","            # Q LEARNING; GET MAXIMUM Q VALUE AT NEXT STAE FROM TRAGET MODEL\n","            if done[i]:\n","                target[i][action[i]] = reward[i]\n","            else:\n","                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))\n","\n","        # MAKE MINI-BATCH WHICH INCLUDES TARGET Q VALUE AND PREDICETED Q VALUE\n","        # where the verbose = 0, without loss values\n","        # where the verbos = 1, with loss values\n","        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)"],"metadata":{"id":"Bb5i93aCjdMt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train\n","--"],"metadata":{"id":"mUP37cYVjpuk"}},{"cell_type":"code","source":["EPISODES = 1000\n","\n","if __name__ == \"__main__\":\n","    env = CartPoleEnv()\n","\n","    # get size of state and action from environment\n","    state_size = env.observation_space.shape[0]\n","    action_size = env.action_space.n\n","\n","    print('observation {}'.format(env.observation_space.shape))\n","    print('state_size {}'.format(state_size))\n","    print('action_size {}'.format(action_size))\n","\n","    agent = DQNAgent(state_size, action_size, test_mode = False)\n","\n","    scores, episodes = [], []\n","\n","    for e in range(EPISODES):\n","        done = False\n","        score = 0\n","        state = env.reset()\n","        state = np.reshape(state, [1, state_size])\n","\n","        while not done:\n","            # get action for the current state and go one step in environment\n","            action = agent.get_action(state)\n","            next_state, reward, done, info = env.step(action)\n","            next_state = np.reshape(next_state, [1, state_size])\n","\n","            # if an action make the episode end, then gives penalty of -100\n","            reward = reward if not done or score == 499 else -100\n","\n","            # save the sample <s, a, r, s'> to the replay memory\n","            agent.append_sample(state, action, reward, next_state, done)\n","            # every time step do the training\n","            agent.train_model()\n","            score += reward\n","            state = next_state\n","\n","            if done:\n","                # every episode update the target model to be same with model\n","                agent.update_target_model()\n","\n","                # every episode, plot the play time\n","                score = score if score == 500 else score + 100\n","                scores.append(score)\n","                episodes.append(e)\n","                print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory), \"  epsilon:\", agent.epsilon)\n","\n","                # if the mean of scores of last 10 episode is bigger than 490 stop training\n","                if np.mean(scores[-min(10, len(scores)):]) > 490:\n","                    sys.exit()\n","\n","        # save the model\n","        if e % 50 == 0:\n","            agent.model.save_weights(\".cartpole.h5\", overwrite=True)"],"metadata":{"id":"H0kuJrI1jrBj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test\n","--"],"metadata":{"id":"Z7X3YfzQjrfa"}},{"cell_type":"code","source":["# custom evaluation function without tensorflow and keras\n","def DRL_evaluate(env, agent):\n","  eval_reward = []\n","  for i in range(5):\n","    obs = env.reset()\n","    episode_reward = 0\n","    while True:\n","      action = agent.get_action(obs)\n","      next_state, reward, done, info = env.step(action)\n","      episode_reward += reward\n","      if done:\n","        break\n","    eval_reward.append(episode_reward)\n","  return np.mean(eval_reward)\n","\n","del agent.model\n","del agent\n","del env\n","\n","env = CartPoleEnv()\n","agent = DQNAgent(state_size, action_size, test_mode = True)\n","\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","\n","res = DRL_evaluate(env, agent)\n","print('acheivable objective function value {}'.format(res))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YR5Cy2QsFpg","executionInfo":{"status":"ok","timestamp":1662184384438,"user_tz":-540,"elapsed":1466,"user":{"displayName":"youngwoo Oh","userId":"06892641019570208070"}},"outputId":"12208319-9956-4cd4-c457-ba8bf684c43f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acheivable objective function value 14.0\n"]}]}]}